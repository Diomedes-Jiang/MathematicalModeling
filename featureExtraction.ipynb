{
 "cells": [
  {
   "cell_type": "raw",
   "source": [
    "##%% md\n",
    "# 词袋模型\n",
    "\n",
    "词袋模型会给每个单词进行编码，而忽略单词的顺序、语法和句法"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fd5c9a623e00909f"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-01-31T06:37:56.908144200Z",
     "start_time": "2024-01-31T06:37:56.787823500Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus=[\n",
    "    'UNC played Duke in basketball',\n",
    "    'Duke lost the basketball game'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 0 1 0 1 0 1]\n",
      " [1 1 1 0 1 0 1 0]]\n",
      "{'unc': 7, 'played': 5, 'duke': 1, 'in': 3, 'basketball': 0, 'lost': 4, 'the': 6, 'game': 2}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "print(vectorizer.fit_transform(corpus).todense())\n",
    "print(vectorizer.vocabulary_)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-31T06:38:01.374983900Z",
     "start_time": "2024-01-31T06:37:56.901887200Z"
    }
   },
   "id": "2632d0604e560370",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "输出结果向量中的1表示词典中该位置的单词出现在了句子中"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1b1d4ab8097a5b00"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 0 1 0 1 0 1]\n",
      " [1 1 1 0 1 0 1 0]\n",
      " [1 1 1 0 1 0 1 0]]\n",
      "{'unc': 7, 'played': 5, 'duke': 1, 'in': 3, 'basketball': 0, 'lost': 4, 'the': 6, 'game': 2}\n"
     ]
    }
   ],
   "source": [
    "corpus.append('Duke lost the game basketball')\n",
    "print(vectorizer.fit_transform(corpus).todense())\n",
    "print(vectorizer.vocabulary_)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-31T06:38:01.377202500Z",
     "start_time": "2024-01-31T06:38:01.363988700Z"
    }
   },
   "id": "fc2229196b09e230",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "注意到，在词袋模型中，即使两个句子的表意不完全相同，如果他们的词汇完全一致，则会被编码成相同的向量"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "445fdc0cb07ae49a"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 0 1 0 1 0 0 1]\n",
      " [0 1 1 1 0 1 0 0 1 0]\n",
      " [0 1 1 1 0 1 0 0 1 0]\n",
      " [1 0 0 0 0 0 0 1 0 0]]\n",
      "{'unc': 9, 'played': 6, 'duke': 2, 'in': 4, 'basketball': 1, 'lost': 5, 'the': 8, 'game': 3, 'ate': 0, 'sandwich': 7}\n"
     ]
    }
   ],
   "source": [
    "corpus.append('I ate a sandwich')\n",
    "print(vectorizer.fit_transform(corpus).todense())\n",
    "print(vectorizer.vocabulary_)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-31T06:38:01.397400400Z",
     "start_time": "2024-01-31T06:38:01.374983900Z"
    }
   },
   "id": "94aed72ae1c14775",
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "可以利用欧几里得范数（$L^2$范数）衡量两个向量的距离，代表着两个句子语义的差距"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e5619815ae0fd6e"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance between each document is [[2.44948974]] and [[0.]], and [[2.64575131]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "import numpy as np\n",
    "X=vectorizer.fit_transform(corpus).todense()\n",
    "X=np.asarray(X)\n",
    "print(f'Distance between each document is {euclidean_distances(X[0].reshape(1,-1),X[1].reshape(1,-1))} and {euclidean_distances(X[1].reshape(1,-1),X[2].reshape(1,-1))}, and {euclidean_distances(X[2].reshape(1,-1),X[3].reshape(1,-1))}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-31T06:38:01.832506900Z",
     "start_time": "2024-01-31T06:38:01.390466700Z"
    }
   },
   "id": "c99292207fa657b8",
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 维度灾难\n",
    "\n",
    "当使用的训练集包括大量的文本资料，同时词汇间的重合度不高时，容易产生一个向量中存在大量的0元素，导致向量列数激增，被称为维度灾难\n",
    "\n",
    "一个解决维度灾难的方法为停用词过滤  \n",
    "比如将大小写合并为小写，同时删除所有的冠词、助动词和介词"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "296cfb939c598f11"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 0 0 1 0 1]\n",
      " [0 1 1 1 1 0 0 0]\n",
      " [0 1 1 1 1 0 0 0]\n",
      " [1 0 0 0 0 0 1 0]]\n",
      "{'unc': 7, 'played': 5, 'duke': 2, 'basketball': 1, 'lost': 4, 'game': 3, 'ate': 0, 'sandwich': 6}\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "print(vectorizer.fit_transform(corpus).todense())\n",
    "print(vectorizer.vocabulary_)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-31T06:38:01.837147300Z",
     "start_time": "2024-01-31T06:38:01.822280600Z"
    }
   },
   "id": "849327a7a489500c",
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "虽然上述方式在一定程度上降低了维度，但是往往效果不好\n",
    "另外两种能够降低维度的方法是**词干提取**和**词干还原**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5a9b744190d764a7"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 1]\n",
      " [0 1 1 0]]\n",
      "{'ate': 0, 'sandwiches': 3, 'sandwich': 2, 'eaten': 1}\n"
     ]
    }
   ],
   "source": [
    "corpus=[\n",
    "    'He ate the sandwiches',\n",
    "    'Every sandwich was eaten by him'\n",
    "]\n",
    "vectorizer=CountVectorizer(binary=True,stop_words='english')\n",
    "print(vectorizer.fit_transform(corpus).todense())\n",
    "print(vectorizer.vocabulary_)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-31T06:38:01.952950800Z",
     "start_time": "2024-01-31T06:38:01.837215800Z"
    }
   },
   "id": "57b62401e4f7fbcb",
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "两个句子的意思类似，但是特征矩阵是正交的，所以应该找到一种方法，将相同词的不同时态统一表示"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "282f868a5cb1f032"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "corpus=[\n",
    "    'I am gathering ingredients for the sandwich.',\n",
    "    'There were many wizards at the gathering'    \n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-31T06:38:01.952950800Z",
     "start_time": "2024-01-31T06:38:01.852564400Z"
    }
   },
   "id": "51fbe3149d330474",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gather\n",
      "gathering\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize('gathering','v'))\n",
    "print(lemmatizer.lemmatize('gathering','n'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-31T06:38:05.396476800Z",
     "start_time": "2024-01-31T06:38:01.861151900Z"
    }
   },
   "id": "4cb992b47b0e96b2",
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "source": [
    "同样的，也可以利用该方法进行词形的还原"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cd0ac18ac0ca1fc9"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gather\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "print(stemmer.stem('gathering'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-31T06:38:05.398539200Z",
     "start_time": "2024-01-31T06:38:05.385172200Z"
    }
   },
   "id": "189d88e08e61ac22",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stemmed:  [['he', 'ate', 'the', 'sandwich'], ['everi', 'sandwich', 'wa', 'eaten', 'by', 'him']]\n"
     ]
    }
   ],
   "source": [
    "# 将上述语料库做词形还原\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "\n",
    "wordnet_tags=['n','v']\n",
    "corpus=[\n",
    "    'He ate the sandwiches',\n",
    "    'Every sandwich was eaten by him'\n",
    "]\n",
    "stemmer=PorterStemmer()\n",
    "print('stemmed: ',[[stemmer.stem(token) for token in word_tokenize(document)] for document in corpus])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-31T06:38:05.483141800Z",
     "start_time": "2024-01-31T06:38:05.397540400Z"
    }
   },
   "id": "e9857b970d312182",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def lemmatize(token,tag):\n",
    "    if tag[0].lower() in wordnet_tags:\n",
    "        return lemmatizer.lemmatize(token,tag[0].lower())\n",
    "    return token"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-31T06:38:05.484186100Z",
     "start_time": "2024-01-31T06:38:05.432019700Z"
    }
   },
   "id": "4242fb44c962c61e",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001B[93maveraged_perceptron_tagger\u001B[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001B[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger')\n  \u001B[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001B[93mtaggers/averaged_perceptron_tagger/averaged_perceptron_tagger.pickle\u001B[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\diomedes/nltk_data'\n    - 'C:\\\\Users\\\\diomedes\\\\.conda\\\\envs\\\\pyCharm\\\\nltk_data'\n    - 'C:\\\\Users\\\\diomedes\\\\.conda\\\\envs\\\\pyCharm\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\diomedes\\\\.conda\\\\envs\\\\pyCharm\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\diomedes\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mLookupError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[15], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m lemmatizer\u001B[38;5;241m=\u001B[39mWordNetLemmatizer()\n\u001B[1;32m----> 2\u001B[0m tagged_corpus\u001B[38;5;241m=\u001B[39m[pos_tag(word_tokenize(document)) \u001B[38;5;28;01mfor\u001B[39;00m document \u001B[38;5;129;01min\u001B[39;00m corpus]\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mLemmatized:\u001B[39m\u001B[38;5;124m'\u001B[39m, [[lemmatize(token,tag) \u001B[38;5;28;01mfor\u001B[39;00m token,tag \u001B[38;5;129;01min\u001B[39;00m document] \u001B[38;5;28;01mfor\u001B[39;00m document \u001B[38;5;129;01min\u001B[39;00m tagged_corpus])\n",
      "Cell \u001B[1;32mIn[15], line 2\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m      1\u001B[0m lemmatizer\u001B[38;5;241m=\u001B[39mWordNetLemmatizer()\n\u001B[1;32m----> 2\u001B[0m tagged_corpus\u001B[38;5;241m=\u001B[39m[pos_tag(word_tokenize(document)) \u001B[38;5;28;01mfor\u001B[39;00m document \u001B[38;5;129;01min\u001B[39;00m corpus]\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mLemmatized:\u001B[39m\u001B[38;5;124m'\u001B[39m, [[lemmatize(token,tag) \u001B[38;5;28;01mfor\u001B[39;00m token,tag \u001B[38;5;129;01min\u001B[39;00m document] \u001B[38;5;28;01mfor\u001B[39;00m document \u001B[38;5;129;01min\u001B[39;00m tagged_corpus])\n",
      "File \u001B[1;32m~\\.conda\\envs\\pyCharm\\Lib\\site-packages\\nltk\\tag\\__init__.py:165\u001B[0m, in \u001B[0;36mpos_tag\u001B[1;34m(tokens, tagset, lang)\u001B[0m\n\u001B[0;32m    140\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpos_tag\u001B[39m(tokens, tagset\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, lang\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124meng\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m    141\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    142\u001B[0m \u001B[38;5;124;03m    Use NLTK's currently recommended part of speech tagger to\u001B[39;00m\n\u001B[0;32m    143\u001B[0m \u001B[38;5;124;03m    tag the given list of tokens.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    163\u001B[0m \u001B[38;5;124;03m    :rtype: list(tuple(str, str))\u001B[39;00m\n\u001B[0;32m    164\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 165\u001B[0m     tagger \u001B[38;5;241m=\u001B[39m _get_tagger(lang)\n\u001B[0;32m    166\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _pos_tag(tokens, tagset, tagger, lang)\n",
      "File \u001B[1;32m~\\.conda\\envs\\pyCharm\\Lib\\site-packages\\nltk\\tag\\__init__.py:107\u001B[0m, in \u001B[0;36m_get_tagger\u001B[1;34m(lang)\u001B[0m\n\u001B[0;32m    105\u001B[0m     tagger\u001B[38;5;241m.\u001B[39mload(ap_russian_model_loc)\n\u001B[0;32m    106\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 107\u001B[0m     tagger \u001B[38;5;241m=\u001B[39m PerceptronTagger()\n\u001B[0;32m    108\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m tagger\n",
      "File \u001B[1;32m~\\.conda\\envs\\pyCharm\\Lib\\site-packages\\nltk\\tag\\perceptron.py:167\u001B[0m, in \u001B[0;36mPerceptronTagger.__init__\u001B[1;34m(self, load)\u001B[0m\n\u001B[0;32m    164\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclasses \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n\u001B[0;32m    165\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m load:\n\u001B[0;32m    166\u001B[0m     AP_MODEL_LOC \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfile:\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(\n\u001B[1;32m--> 167\u001B[0m         find(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtaggers/averaged_perceptron_tagger/\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m PICKLE)\n\u001B[0;32m    168\u001B[0m     )\n\u001B[0;32m    169\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mload(AP_MODEL_LOC)\n",
      "File \u001B[1;32m~\\.conda\\envs\\pyCharm\\Lib\\site-packages\\nltk\\data.py:583\u001B[0m, in \u001B[0;36mfind\u001B[1;34m(resource_name, paths)\u001B[0m\n\u001B[0;32m    581\u001B[0m sep \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m*\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m70\u001B[39m\n\u001B[0;32m    582\u001B[0m resource_not_found \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00msep\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mmsg\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00msep\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m--> 583\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mLookupError\u001B[39;00m(resource_not_found)\n",
      "\u001B[1;31mLookupError\u001B[0m: \n**********************************************************************\n  Resource \u001B[93maveraged_perceptron_tagger\u001B[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001B[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger')\n  \u001B[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001B[93mtaggers/averaged_perceptron_tagger/averaged_perceptron_tagger.pickle\u001B[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\diomedes/nltk_data'\n    - 'C:\\\\Users\\\\diomedes\\\\.conda\\\\envs\\\\pyCharm\\\\nltk_data'\n    - 'C:\\\\Users\\\\diomedes\\\\.conda\\\\envs\\\\pyCharm\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\diomedes\\\\.conda\\\\envs\\\\pyCharm\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\diomedes\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "lemmatizer=WordNetLemmatizer()\n",
    "tagged_corpus=[pos_tag(word_tokenize(document)) for document in corpus]\n",
    "print('Lemmatized:', [[lemmatize(token,tag) for token,tag in document] for document in tagged_corpus])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-31T06:38:06.052440400Z",
     "start_time": "2024-01-31T06:38:05.441876900Z"
    }
   },
   "id": "80cf7bec7e6e8859",
   "execution_count": 15
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
